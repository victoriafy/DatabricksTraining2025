
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ›ï¸ Order Enrichment and Regional Analysis\n",
    "### Scenario:\n",
    "You are a data engineer at an e-commerce company. You are given an existing Delta table of orders and a new CSV file containing customer data. Your task is to enrich the order table by joining it with the customer info, overwrite the Delta table with the enriched data, perform some analysis, and explore Delta Lake's time travel features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load the Existing Delta Table\n",
    "orders_path = "/dbfs/tmp/datatraining/orders_delta"\n",
    "orders_df = spark.read.format("delta").load(orders_path)\n",
    "orders_df.printSchema()\n",
    "orders_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Ingest the Customer CSV File\n",
    "customers_path = "/dbfs/tmp/datatraining/customers.csv"\n",
    "customers_df = spark.read.option("header", True).option("inferSchema", True).csv(customers_path)\n",
    "customers_df.printSchema()\n",
    "customers_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Join Orders with Customers\n",
    "enriched_orders_df = orders_df.join(customers_df, on="customer_id", how="left")\n",
    "enriched_orders_df.select("order_id", "customer_id", "customer_name", "region", "order_amount").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Overwrite the Delta Table\n",
    "enriched_orders_df.write.format("delta").mode("overwrite").save(orders_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Perform Analysis - Orders by Region\n",
    "from pyspark.sql.functions import count, sum\n",
    "region_summary = enriched_orders_df.groupBy("region").agg(\n",
    "    count("order_id").alias("total_orders"),\n",
    "    sum("order_amount").alias("total_sales")\n",
    ").orderBy("total_sales", ascending=False)\n",
    "region_summary.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ§  Bonus Tasks\n",
    "Try solving these using PySpark DataFrame API:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. What region had the most orders?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. What was the most popular product category?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. How many customers signed up in 2023?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. What is the average order value per region?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. Find top 3 customers with the highest total spending.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6. Which month had the highest number of orders overall? (Hint: extract month from date)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7. Challenge: Create a column that labels orders as 'High Value' if order_amount > 500, then count how many high-value orders per region.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Explore Delta Table History\n",
    "display(spark.sql(f"DESCRIBE HISTORY delta.`{orders_path}`"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Rollback to Previous Version (Optional)\n",
    "old_version_df = spark.read.format("delta").option("versionAsOf", 0).load(orders_path)\n",
    "old_version_df.show(5)\n",
    "# Optionally, you can roll back by overwriting again:\n",
    "# old_version_df.write.format("delta").mode("overwrite").save(orders_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## âœ… Summary\n",
    "- You loaded a Delta table and a CSV\n",
    "- Joined them using PySpark\n",
    "- Overwrote a Delta table\n",
    "- Performed grouped analysis\n",
    "- Used Delta Lake's time travel feature\n",
    "\n",
    "ðŸŽ‰ Great work! Try exploring with more complex joins or aggregations next."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
